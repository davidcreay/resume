personal:
  name: David Reay
  first_name: David
  last_name: Reay
  title: Senior Data Engineer | Python & AWS Architect
  summary: >
    Senior Data Engineer and AWS Professional Architect with over 15 years of experience. Expert in building scalable data backbones, designing production-grade ETL/ELT pipelines, and enabling better decision-making through robust data architecture. Proven track record in regulated industries (Finance, Government, Healthcare) delivering trusted datasets, modernizing data warehouses to Data Lakes, and preparing infrastructure for future ML/AI use cases.
  email: 'david@awsbot.com'
  phone: '+44 (0) 7834 980 889'
  location: London, United Kingdom
  github: https://github.com/awsbot-ltd
  linkedin: davidreay
  website: https://forhire.awsbot.com
  nationality: British
  security_clearance: BPSS (Active)
  interests:
    - title: Music & Arts
      description: Jazz piano, classical guitar, and graphic design.
    - title: Game Theory & AI
      description: Contributor to GNUBG project; specialized in Neural Network integration.

languages:
  - name: Python
    level: Expert
    rating: 5
  - name: AWS
    level: Professional
    rating: 5

achievements:
  - icon: '\faServer'
    title: 'Cloud Data Architecture'
    description: 'Architected secure, multi-account AWS Landing Zones and Data Lakes for Giacom, utilizing Glue, Lambda, and Terraform to support analytics.'
  - icon: '\faStream'
    title: 'Streaming & Big Data'
    description: 'Implemented Big Data pipelines using AirFlow, Kinesis, and Firehose for global data aggregation, replacing legacy infrastructure.'
  - icon: '\faMicrochip'
    title: 'PySpark & Data Quality'
    description: 'Engineered a de-identification pipeline using PySpark and NLP for Mental Health Innovations, ensuring data quality and privacy compliance.'
  - icon: '\faCogs'
    title: 'Serverless ETL'
    description: 'Built complex Python serverless orchestration systems for Chetwood Financial, integrating 3rd party APIs for real-time financial checks.'

skill_groups:
  # Data Engineering & Analytics
  - Python
  - SQL / NoSQL
  - AWS Glue
  - AWS Athena
  - PySpark
  - ETL/ELT Pipelines
  - Data Warehousing
  - Data Lakes

  # Cloud & Infrastructure (AWS)
  - AWS Lambda
  - AWS Step Functions
  - Amazon DynamoDB
  - Kinesis / Firehose
  - CloudFormation
  - Terraform
  - Boto3

  # AI & Automation
  - Amazon Bedrock
  - OpenAI API
  - NLP (SpaCy)
  - Automation

  # Engineering Practices
  - CI/CD (Jenkins/Git)
  - Stakeholder Mgmt
  - Agile/Scrum
  - DevSecOps
  - TDD

experience:
  - company: 'Giacom (Communications) Limited'
    logo: giacom_logo.jpeg
    location: 'Southampton, UK'
    location_type: 'Remote'
    employment_type: 'Contract'
    position: 'Senior Data Engineer'
    current: false
    start_date:
      month: 'April'
      year: '2024'
    end_date:
      month: 'February'
      year: '2025'
    description: >
      Designed and implemented a scalable data backbone for AWS solutions. Focused on connecting multiple data sources and improving data reliability for reporting and future AI integration.
    achievements:
      - Architected multi-account AWS Landing Zones and Data Lakes to enable cross-business analytics.
      - Developed automated CI/CD pipelines for data ingestion using Python, AWS Glue, and Lambda.
      - Mentored junior staff on AWS Well-Architected Framework and modern data engineering practices.
    responsibilities:
      - Designing data architecture and maintaining pipelines (ingestion, transformation) using AWS data stack.
      - Writing Infrastructure as Code (Terraform) for VPC segmentation and security.
    skills:
      - 'Python'
      - 'AWS Glue'
      - 'Data Lakes'
      - 'Terraform'

  - company: 'Mental Health Innovations UK'
    logo: mental_health_innovations_logo.jpeg
    location: 'London, UK'
    location_type: 'Remote'
    employment_type: 'Contract'
    position: 'Python Data Engineer'
    current: false
    start_date:
      month: 'May'
      year: '2023'
    end_date:
      month: 'January'
      year: '2024'
    description: 'Contract to develop a high-performance data transformation pipeline using Python and PySpark to ensure data quality and privacy.'
    achievements:
      - Developed and optimized ETL pipelines using PySpark and NLP to extract and anonymize sensitive entities.
      - Implemented rigorous TDD methodologies using pytest to ensure data reliability.
      - Optimized pipeline performance for ARM64 AWS Graviton processors to reduce compute costs.
    responsibilities:
      - Python development for data transformation and quality assurance.
      - Infrastructure as Code with CloudFormation and Dockerization (EMR Serverless).
    skills:
      - 'Python (PySpark)'
      - 'ETL'
      - 'TDD'
      - 'EMR'

  - company: 'Nationale-Nederlanden Investment Partners'
    logo: nnip.jpeg
    location: 'The Hague, Netherlands'
    location_type: 'Remote'
    employment_type: 'Contract'
    position: 'Senior Data Engineer'
    current: false
    start_date:
      month: 'April'
      year: '2021'
    end_date:
      month: 'March'
      year: '2022'
    description: 'Developed Python-based back-office tools for assessing and reporting on financial risk, ensuring data accuracy for asset management.'
    achievements:
      - Built ETL workflows using AWS Glue and Athena for risk measurement and reporting.
      - Utilized Python libraries (Pandas, NumPy, Boto3) for complex data manipulation and SQL analysis.
    responsibilities:
      - Serverless development (Lambda, Step Functions, DynamoDB).
      - Ensuring data structure and accessibility support reporting/insight tools.
    skills:
      - 'Python (Pandas)'
      - 'AWS Glue'
      - 'SQL/Athena'

  - company: 'Chetwood Financial Ltd'
    logo: chetwood.jpeg
    location: 'Wrexham, Wales'
    location_type: 'Remote'
    employment_type: 'Contract'
    position: 'Senior Python Developer'
    current: false
    start_date:
      month: 'September'
      year: '2020'
    end_date:
      month: 'March'
      year: '2021'
    description: 'Architected a complex serverless journey management system for credit card applications, orchestrating data flow between internal systems and 3rd party providers.'
    achievements:
      - Built a serverless orchestration engine using AWS Step Functions and Lambda.
      - Integrated external APIs (Onfido, Experian) for real-time fraud/credit checks.
    responsibilities:
      - Writing clean, maintainable Python code with Boto3.
      - API Gateway configuration and secure data handling.
    skills:
      - 'Python'
      - 'AWS Step Functions'
      - 'API Integration'

  - company: 'GFK Nurago'
    logo: gfk.jpeg
    location: 'Berlin, Germany'
    location_type: 'Hybrid'
    employment_type: 'Contract'
    position: 'Team Lead / Data Engineer'
    current: false
    start_date:
      month: 'July'
      year: '2017'
    end_date:
      month: 'October'
      year: '2019'
    description: 'Led the migration of a global suite of panellist data aggregation tracking tools from legacy infrastructure to AWS.'
    achievements:
      - Implemented Big Data pipelines using AirFlow, Kinesis, and Firehose for streaming data ingestion.
      - Managed infrastructure orchestration with Terraform and CloudFormation.
    responsibilities:
      - Design and delivery of cloud data platform evolution.
      - Monitoring and operations using Grafana and Prometheus.
    skills:
      - 'Kinesis (Streaming)'
      - 'AirFlow'
      - 'Big Data'

  - company: 'AstraZeneca'
    logo: astrazeneca_logo.jpeg
    location: 'Cambridge, UK'
    location_type: 'Remote'
    employment_type: 'Contract'
    position: 'Senior DevOps Engineer'
    current: false
    start_date:
      month: 'August'
      year: '2022'
    end_date:
      month: 'December'
      year: '2022'
    description: 'Deployed bioinformatics build-pipelines to sequence DNA data for R&D oncology.'
    achievements:
      - Developed custom AWS Data Lake components using Python, Lambda, and Step Functions (Postgres/RDS).
      - Automated specialized bioinformatics tools via Ansible.
    responsibilities:
      - CI/CD pipeline development and data infrastructure security.
    skills:
      - 'Data Lakes'
      - 'Postgres (RDS)'
      - 'Python'

education:
  - degree: MSc Artificial Intelligence
    institution: University of Hertfordshire
    year: 2025 – 2027
  - degree: BSc in Physical Science (First-Class Honours)
    institution: The Open University
    year: 2006 – 2011

certifications_and_training:
  - name: 'Solutions Architect - Professional'
    issuer: 'Amazon Web Services'
    category: 'Cloud Architecture'
  - name: 'DevOps Engineer - Professional'
    issuer: 'Amazon Web Services'
    category: 'Cloud Architecture'
  - name: 'SAFe 4 Practitioner'
    issuer: 'Scaled Agile'
    category: 'Agile'
